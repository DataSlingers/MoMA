<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>MoMA - Modern Multivariate Analysis in R • MoMA</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.3.1/jquery.min.js" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://cdnjs.cloudflare.com/ajax/libs/bootswatch/3.3.7/flatly/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha256-U5ZEeKfGNOja007MMD3YBI0A3OSZOQbeG6z2f2Y0hu8=" crossorigin="anonymous"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css" integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.4/clipboard.min.js" integrity="sha256-FiZwavyI2V6+EXO1U+xzLG3IKldpiTFf3153ea9zikQ=" crossorigin="anonymous"></script><!-- sticky kit --><script src="https://cdnjs.cloudflare.com/ajax/libs/sticky-kit/1.1.3/sticky-kit.min.js" integrity="sha256-c4Rlo1ZozqTPE2RLuvbusY3+SU1pQaJC0TjuhygMipw=" crossorigin="anonymous"></script><!-- pkgdown --><link href="pkgdown.css" rel="stylesheet">
<script src="pkgdown.js"></script><meta property="og:title" content="MoMA - Modern Multivariate Analysis in R">
<meta property="og:description" content="Unified approach to modern multivariate analysis providing sparse,
    smooth, and structured versions of PCA, PLS, LDA, and CCA.">
<meta name="twitter:card" content="summary">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body>
    <div class="container template-article">
      <header><div class="navbar navbar-inverse navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="index.html">MoMA</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="Released version">0.1</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="articles/moma-quick-start.html">Intro</a>
</li>
<li>
  <a href="reference/index.html">Reference</a>
</li>
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Articles
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="articles/moma-functional-data-analysis.html">Funtional Data Analysis</a>
    </li>
    <li>
      <a href="articles/moma-PCA.html">Principal Component Anlaysis</a>
    </li>
    <li>
      <a href="articles/moma-LDA.html">Linear Discriminant Analysis</a>
    </li>
  </ul>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/DataSlingers/MoMA">
    <span class="fa fa-github fa-lg"></span>
     
  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9 contents">
    

    
    
<!--  -*- coding: utf-8 -*- -->
<!-- README.md is generated from README.Rmd. Do not edit this file directly -->
<div id="moma-modern-multivariate-analysis" class="section level1">
<div class="page-header"><h1 class="hasAnchor">
<a href="#moma-modern-multivariate-analysis" class="anchor"></a>MoMA: Modern Multivariate Analysis</h1></div>
<!-- badges: start -->

<p>MoMA is a penalized SVD framework that supports a wide range of sparsity-inducing penalties. For a matrix <em>X</em>, MoMA gives the solution to the following optimization problem:</p>
<center>
<img src="reference/figures/moma-formula.svg" style="width: 500; max-width: 100%; height: auto" title="MoMA formula"><br>
</center>
<p>The penalties (the <em>P</em> functions) we support so far include</p>
<ul>
<li><p><code><a href="reference/lasso.html">moma_lasso()</a></code>: LASSO (least absolute shrinkage and selection operator).</p></li>
<li><p><code><a href="reference/scad.html">moma_scad()</a></code>: SCAD (smoothly clipped absolute deviation).</p></li>
<li><p><code><a href="reference/mcp.html">moma_mcp()</a></code> MCP (minimax concave penalty).</p></li>
<li><p><code><a href="reference/slope.html">moma_slope()</a></code>: SLOPE (sorted <span class="math inline">\(\ell\)</span>-one penalized estimation).</p></li>
<li><p><code><a href="reference/group_lasso.html">moma_grplasso()</a></code>: Group LASSO.</p></li>
<li><p><code><a href="reference/fused_lasso.html">moma_fusedlasso()</a></code>: Fused LASSO.</p></li>
<li><p><code><a href="reference/sparse_fused_lasso.html">moma_spfusedlasso()</a></code>: Sparse fused LASSO.</p></li>
<li><p><code><a href="reference/l1_trend_filtering.html">moma_l1tf()</a></code>: <span class="math inline">\(\ell\)</span>-one trend filtering.</p></li>
<li><p><code><a href="reference/cluster.html">moma_cluster()</a></code>: Cluster penalty.</p></li>
</ul>
<p>With this at hand, we can easily extend classical multivariate models:</p>
<ul>
<li><p><code><a href="reference/moma_sfpca.html">moma_sfpca()</a></code> performs penalized principal component analysis.</p></li>
<li><p><code><a href="reference/moma_sfcca.html">moma_sfcca()</a></code> performs penalized canonical component analysis.</p></li>
<li><p><code><a href="reference/moma_sflda.html">moma_sflda()</a></code> performs penalized linear discriminant analysis.</p></li>
</ul>
<p>We also provide Shiny App support to facilitate interaction with the results. If you are new to MoMA, the best place to start is <code>vignette("MoMA")</code>.</p>
<div id="installation" class="section level2">
<h2 class="hasAnchor">
<a href="#installation" class="anchor"></a>Installation</h2>
<p>The newest version of the package can be installed from Github:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb1-1" data-line-number="1"><span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/library">library</a></span>(devtools)</a>
<a class="sourceLine" id="cb1-2" data-line-number="2"><span class="kw"><a href="https://www.rdocumentation.org/packages/devtools/topics/reexports">install_github</a></span>(<span class="st">"DataSlingers/MoMA"</span>, <span class="dt">ref =</span> <span class="st">"master"</span>)</a></code></pre></div>
</div>
<div id="usage" class="section level2">
<h2 class="hasAnchor">
<a href="#usage" class="anchor"></a>Usage</h2>
<p>Perform sparse linear discriminant analysis on the Iris data set.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb2-1" data-line-number="1"><span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/library">library</a></span>(MoMA)</a>
<a class="sourceLine" id="cb2-2" data-line-number="2"></a>
<a class="sourceLine" id="cb2-3" data-line-number="3">## collect data</a>
<a class="sourceLine" id="cb2-4" data-line-number="4">X &lt;-<span class="st"> </span>iris[, <span class="dv">1</span><span class="op">:</span><span class="dv">4</span>]</a>
<a class="sourceLine" id="cb2-5" data-line-number="5">Y_factor &lt;-<span class="st"> </span><span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/factor">as.factor</a></span>(<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/rep">rep</a></span>(<span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/c">c</a></span>(<span class="st">"s"</span>, <span class="st">"c"</span>, <span class="st">"v"</span>), <span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/rep">rep</a></span>(<span class="dv">50</span>, <span class="dv">3</span>)))</a>
<a class="sourceLine" id="cb2-6" data-line-number="6"></a>
<a class="sourceLine" id="cb2-7" data-line-number="7">## range of penalty</a>
<a class="sourceLine" id="cb2-8" data-line-number="8">lambda &lt;-<span class="st"> </span><span class="kw"><a href="https://www.rdocumentation.org/packages/base/topics/seq">seq</a></span>(<span class="dv">0</span>, <span class="dv">1</span>, <span class="fl">0.1</span>)</a>
<a class="sourceLine" id="cb2-9" data-line-number="9"></a>
<a class="sourceLine" id="cb2-10" data-line-number="10">## run!</a>
<a class="sourceLine" id="cb2-11" data-line-number="11">a &lt;-<span class="st"> </span><span class="kw"><a href="reference/moma_sflda.html">moma_sflda</a></span>(</a>
<a class="sourceLine" id="cb2-12" data-line-number="12">    <span class="dt">X =</span> X,</a>
<a class="sourceLine" id="cb2-13" data-line-number="13">    <span class="dt">Y_factor =</span> Y_factor,</a>
<a class="sourceLine" id="cb2-14" data-line-number="14">    <span class="dt">x_sparse =</span> <span class="kw"><a href="reference/lasso.html">moma_lasso</a></span>(<span class="dt">lambda =</span> lambda),</a>
<a class="sourceLine" id="cb2-15" data-line-number="15">    <span class="dt">rank =</span> <span class="dv">3</span></a>
<a class="sourceLine" id="cb2-16" data-line-number="16">)</a>
<a class="sourceLine" id="cb2-17" data-line-number="17"></a>
<a class="sourceLine" id="cb2-18" data-line-number="18"><span class="kw"><a href="https://www.rdocumentation.org/packages/graphics/topics/plot">plot</a></span>(a) <span class="co"># start a Shiny app and play with it!</span></a></code></pre></div>
<center>
<img src="reference/figures/README-usage-ldademo.png" style="width: 500px; max-width: 100%; height: auto" title="Shiny app">
</center>
</div>
<div id="background" class="section level2">
<h2 class="hasAnchor">
<a href="#background" class="anchor"></a>Background</h2>
<p>Multivariate analysis – the study of finding meaningful patterns in datasets – is a key technique in any data scientist’s toolbox. Beyond its use for Exploratory Data Analysis (“EDA”), multivariate analysis also allows for principled <em>Data-Driven Discovery</em>: finding meaningful, actionable, and reproducible structure in large data sets. Classical techniques for multivariate analysis have proven immensely successful through history, but modern Data-Driven Discovery requires new techniques to account for the specific complexities of modern data. This package provides a new unified framework for <strong>Modern Multivariate Analysis</strong> (“MoMA”), which will provide a unified and flexible baseline for future research in multivariate analysis. Even more importantly, we anticipate that this easy-to-use <code>R</code> package will increase adoption of these powerful new models by end users and, in conjunction with <code>R</code>’s rich graphics libraries, position <code>R</code> as the leading platform for modern exploratory data analysis and data-driven discovery.</p>
<p>Multivariate analysis techniques date back to the earliest days of statistics, pre-dating other foundational concepts like hypothesis testing by several decades. Classical techniques such as Principal Components Analysis (“PCA”) [1, 2], Partial Least Squares (“PLS”), Canonical Correlation Analysis (“CCA”) [3], and Linear Discriminant Analysis (“LDA”), have a long and distinguished history of use in statistics and are still among the most widely used methods for EDA. Their importance is reflected in the CRAN Task View dedicated to Multivariate Analysis [4], as well as the specialized implementations available for a range of application areas. Somewhat surprisingly, each of these techniques can be interpreted as a variant of the well-studied eigendecomposition problem, allowing statisticians to build upon a rich mathematical and computational literature.</p>
<p>In the early 2000s, researchers noted that naive extensions of classical multivariate techniques to the high-dimensional setting produced unsatisfactory results, a finding later confirmed by advances in random matrix theory [5]. In response to these findings, multivariate analysis experienced a renaissance as researchers developed a wide array of new techniques to incorporate sparsity, smoothness, and other structure into classical techniques [6,7,8,9,10,11,12,13,14 among many others], resulting in a rich literature on “modern multivariate analysis.” Around the same time, theoretical advances showed that these techniques avoided many of the pitfalls associated with naive extensions [15,16,17,18,19,20].</p>
<p>While this literature is vast, it relies on a single basic principle: it is essential to adapt classical techniques to account for known characteristics and complexities of the dataset at hand for multivariate analysis to succeed. For example, a neuroscientist investigating the brain’s response to an external stimulus may expect a response which is simultaneously spatially smooth and sparse: spatially smooth because the brain processes related stimuli in well-localized areas (<em>e.g.</em>, the visual cortex) and sparse because not all regions of the brain are used to respond to a given stimulus. Alternatively, a statistical factor model used to understand financial returns may be significantly improved by incorporating known industry sector data, motivating a form of group sparsity. A sociologist studying how pollution leads to higher levels of respiratory illnesses may combine spatial smoothness and sparsity (indicating “pockets” of activity) with a non-negativity constraint, knowing that pollution and illness have a positive effect.</p>
<p>To incorporate these different forms of prior knowledge into multivariate analysis, a wide variety of algorithms and approaches have been proposed. In 2013, Allen proposed a general framework that unified existing techniques for “modern” PCA, as well as proposing a number of novel extensions [21]. The recently developed <code>MoMA</code> algorithm builds on this work, allowing more forms of regularization and structure, as well as supporting more forms of multivariate analysis.</p>
<p>The principal aim of this package is to make modern multivariate analysis available to a wide audience. This package will allow for fitting PCA, PLS, CCA, and LDA with all of the modern “bells-and-whistles:” sparsity, smoothness, ordered and unordered fusion, orthogonalization with respect to arbitrary bases, and non-negativity constraints. Uniting this wide literature under a single umbrella using the <code>MoMA</code> algorithm will provide a unified and flexible platform for data-driven discovery in <code>R</code>.</p>
</div>
<div id="authors" class="section level2">
<h2 class="hasAnchor">
<a href="#authors" class="anchor"></a>Authors</h2>
<ul>
<li>
<p><a href="http://github.com/michaelweylandt">Michael Weylandt</a></p>
<p>Department of Statistics, Rice University</p>
</li>
<li>
<p><a href="http://www.stat.rice.edu/~gallen">Genevera Allen</a></p>
<p>Departments of Statistics, CS,and ECE, Rice University</p>
<p>Jan and Dan Duncan Neurological Research Institute Baylor College of Medicine and Texas Children’s Hospital</p>
</li>
<li>
<p><a href="http://github.com/Banana1530">Luofeng “Luke” Liao</a></p>
<p>School of Data Science, Fudan University</p>
</li>
</ul>
</div>
<div id="acknowledgements" class="section level2">
<h2 class="hasAnchor">
<a href="#acknowledgements" class="anchor"></a>Acknowledgements</h2>
<ul>
<li>MW was funded by an NSF Graduate Research Fellowship 1450681.</li>
<li>LL was funded by Google Summer of Code 2019.</li>
</ul>
</div>
<div id="references" class="section level2">
<h2 class="hasAnchor">
<a href="#references" class="anchor"></a>References</h2>
<p>[1] K. Pearson. “On Lines and Planes of Closest Fit to Systems of Points in Space.” The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science 2, p.559-572, 1901. <a href="https://doi.org/10.1080/14786440109462720" class="uri">https://doi.org/10.1080/14786440109462720</a></p>
<p>[2] H. Hotelling. Analysis of a Complex of Statistical Variables into Principal Components. Journal of Educational Psychology 24(6), p.417-441, 1933. <a href="http://dx.doi.org/10.1037/h0071325" class="uri">http://dx.doi.org/10.1037/h0071325</a></p>
<p>[3] H. Hotelling. “Relations Between Two Sets of Variates” Biometrika 28(3-4), p.321-377, 1936. <a href="https://doi.org/10.1093/biomet/28.3-4.321" class="uri">https://doi.org/10.1093/biomet/28.3-4.321</a></p>
<p>[4] See <a href="https://cran.r-project.org/web/views/Multivariate.html">CRAN Task View: Multivariate Statistics</a></p>
<p>[5] I. Johnstone, A. Lu. “On Consistency and Sparsity for Principal Components Analysis in High Dimensions.” Journal of the American Statistical Association: Theory and Methods 104(486), p.682-693, 2009. <a href="https://doi.org/10.1198/jasa.2009.0121" class="uri">https://doi.org/10.1198/jasa.2009.0121</a></p>
<p>[6] B. Silverman. “Smoothed Functional Frincipal Fomponents Analysis by Choice of Norm.” Annals of Statistics 24(1), p.1-24, 1996. <a href="https://projecteuclid.org/euclid.aos/1033066196" class="uri">https://projecteuclid.org/euclid.aos/1033066196</a></p>
<p>[7] J. Huang, H. Shen, A. Buja. “Functional Principal Components Analysis via Penalized Rank One Approximation.” Electronic Journal of Statistics 2, p.678-695, 2008. <a href="https://projecteuclid.org/euclid.ejs/1217450800" class="uri">https://projecteuclid.org/euclid.ejs/1217450800</a></p>
<p>[8] I.T. Jolliffe, N.T. Trendafilov, M. Uddin. “A Modified Principal Component Technique Based on the Lasso.” Journal of Computational and Graphical Statistics 12(3), p.531-547, 2003. <a href="https://doi.org/10.1198/1061860032148" class="uri">https://doi.org/10.1198/1061860032148</a></p>
<p>[9] H. Zou, and T. Hastie, and R. Tibshirani. “Sparse Principal Component Analysis.” Journal of Computational and Graphical Statistics 15(2), p.265-286, 2006. <a href="https://doi.org/10.1198/106186006X113430" class="uri">https://doi.org/10.1198/106186006X113430</a></p>
<p>[10] A. d’Aspremont, L. El Gahoui, M.I. Jordan, G.R.G. Lanckriet. “A Direct Formulation for Sparse PCA Using Semidefinite Programming.” SIAM Review 49(3), p.434-448, 2007. <a href="https://doi.org/10.1137/050645506" class="uri">https://doi.org/10.1137/050645506</a></p>
<p>[11] A. d’Aspremont, F. Bach, L. El Gahoui. “Optimal Solutions for Sparse Principal Component Analysis.” Journal of Machine Learning Research 9, p.1269-1294, 2008. <a href="http://www.jmlr.org/papers/v9/aspremont08a.htm" class="uri">http://www.jmlr.org/papers/v9/aspremont08a.htm</a></p>
<p>[12] D. Witten, R. Tibshirani, T. Hastie. “A Penalized Matrix Decomposition, with Applications to Sparse Principal Components and Canonical Correlation Analysis.” Biostatistics 10(3), p.515-534, 2009. <a href="https://doi.org/10.1093/biostatistics/kxp008" class="uri">https://doi.org/10.1093/biostatistics/kxp008</a></p>
<p>[13] R. Jenatton, G. Obozinski. F. Bach. “Structured Sparse Principal Component Analysis.” Proceedings of the 13th International Conference on Artificial Intelligence and Statistics (AISTATS) 2010. <a href="http://proceedings.mlr.press/v9/jenatton10a.html" class="uri">http://proceedings.mlr.press/v9/jenatton10a.html</a></p>
<p>[14] G.I. Allen, M. Maletic-Savatic. “Sparse Non-Negative Generalized PCA with Applications to Metabolomics.” Bioinformatics 27(21), p.3029-3035, 2011. <a href="https://doi.org/10.1093/bioinformatics/btr522" class="uri">https://doi.org/10.1093/bioinformatics/btr522</a></p>
<p>[15] A.A. Amini, M.J. Wainwright. “High-Dimensional Analysis of Semidefinite Relaxations for Sparse Principal Components.” Annals of Statistics 37(5B), p.2877-2921, 2009. <a href="https://projecteuclid.org/euclid.aos/1247836672" class="uri">https://projecteuclid.org/euclid.aos/1247836672</a></p>
<p>[16] S. Jung, J.S. Marron. “PCA Consistency in High-Dimension, Low Sample Size Context.” Annals of Statistics 37(6B), p.4104-4130, 2009. <a href="https://projecteuclid.org/euclid.aos/1256303538" class="uri">https://projecteuclid.org/euclid.aos/1256303538</a></p>
<p>[17] Z. Ma. “Sparse Principal Component Analysis and Iterative Thresholding.” Annals of Statistics 41(2), p.772-801, 2013. <a href="https://projecteuclid.org/euclid.aos/1368018173" class="uri">https://projecteuclid.org/euclid.aos/1368018173</a></p>
<p>[18] T.T. Cai, Z. Ma, Y. Wu. “Sparse PCA: Optimal Rates and Adaptive Estimation.” Annals of Statistics 41(6), p.3074-3110, 2013. <a href="https://projecteuclid.org/euclid.aos/1388545679" class="uri">https://projecteuclid.org/euclid.aos/1388545679</a></p>
<p>[19] V.Q. Vu, J. Lei. “Minimax Sparse Principal Subspace Estimation in High Dimensions.” Annals of Statistics 41(6), p.2905-2947, 2013. <a href="https://projecteuclid.org/euclid.aos/1388545673" class="uri">https://projecteuclid.org/euclid.aos/1388545673</a></p>
<p>[20] D. Shen, H. Shen, J.S. Marron. “Consistency of Sparse PCA in High Dimension, Low Sample Size Contexts.” Journal of Multivariate Analysis 115, p.317-333, 2013. <a href="https://doi.org/10.1016/j.jmva.2012.10.007" class="uri">https://doi.org/10.1016/j.jmva.2012.10.007</a></p>
<p>[21] G.I. Allen. “Sparse and Functional Principal Components Analysis.” ArXiv Pre-Print 1309.2895 (2013). <a href="https://arxiv.org/abs/1309.2895" class="uri">https://arxiv.org/abs/1309.2895</a></p>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
    <div class="links">
<h2>Links</h2>
<ul class="list-unstyled">
<li>Browse source code at <br><a href="https://github.com/DataSlingers/MoMA">https://​github.com/​DataSlingers/​MoMA</a>
</li>
<li>Report a bug at <br><a href="https://github.com/DataSlingers/MoMA/issues">https://​github.com/​DataSlingers/​MoMA/​issues</a>
</li>
</ul>
</div>
<div class="license">
<h2>License</h2>
<ul class="list-unstyled">
<li>GPL (&gt;= 2)</li>
</ul>
</div>
<div class="developers">
<h2>Developers</h2>
<ul class="list-unstyled">
<li>Michael Weylandt <br><small class="roles"> Author, maintainer </small>  </li>
<li>Genevera Allen <br><small class="roles"> Author </small>  </li>
<li>Luofeng Liao <br><small class="roles"> Author </small>  </li>
<li><a href="authors.html">All authors...</a></li>
</ul>
</div>

      <div class="dev-status">
<h2>Dev status</h2>
<ul class="list-unstyled">
<li><a href="https://travis-ci.com/DataSlingers/MoMA"><img src="https://travis-ci.com/DataSlingers/MoMA.svg?branch=develop" alt="TravisCI Build Status"></a></li>
<li><a href="https://codecov.io/gh/DataSlingers/MoMA/branch/develop"><img src="https://codecov.io/gh/DataSlingers/MoMA/branch/develop/graph/badge.svg" alt="codecov Coverage Status"></a></li>
<li><a href="https://www.gnu.org/licenses/old-licenses/gpl-2.0.en.html"><img src="https://img.shields.io/badge/License-GPL%20v2-brightgreen.svg" alt="License: GPL v2"></a></li>
<li><a href="https://cran.r-project.org/package=MoMA"><img src="http://www.r-pkg.org/badges/version/MoMA" alt="CRAN_Status_Badge"></a></li>
<li><a href="http://www.repostatus.org/#wip"><img src="http://www.repostatus.org/badges/latest/wip.svg" alt="Project Status: WIP – Initial development is in progress, but there has not yet been a stable, usable release suitable for the public."></a></li>
</ul>
</div>
</div>

</div>


      <footer><div class="copyright">
  <p>Developed by Michael Weylandt, Genevera Allen, Luofeng Liao.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="https://pkgdown.r-lib.org/">pkgdown</a> 1.3.0.</p>
</div>
      </footer>
</div>

  

  </body>
</html>
