% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/moma_arguments.R
\name{fused_lasso}
\alias{fused_lasso}
\alias{moma_fusedlasso}
\title{Fused LASSO}
\usage{
moma_fusedlasso(..., algo = c("path", "dp"), ..., lambda = 0,
  select_scheme = "g")
}
\arguments{
\item{...}{Forces users to specify all arguments by name.}

\item{algo}{A string being either "path" or "dp". Defaults to "path". Partial matching
is supported. Two solving algorithms
are provided. When "path" is chosen, the algorithm by
Hoefling, H. (2010) is used. When "dp" is chosen, the algorithm by Johnson, N. A. (2013) is used.}

\item{lambda}{A vector containing penalty values}

\item{select_scheme}{A char being either "b" (nested BIC search) or "g" (grid search).

MoMA provides a flexible framework for regularized multivariate analysis
with several tuning parameters for different forms of regularization.
To assist the user in selecting these parameters (\code{alpha_u},
\code{alpha_v}, \code{lambda_u}, \code{lambda_v}), we provide
two selection modes: grid search ("g") and nested BIC search ("b").
Grid search means we solve the problem
for all combinations of parameter values provided by the user.

To explain nested BIC search, we need to look into how the algorithm runs.
To find an (approximate) solution to a penalized SVD (Singular Value Decomposition) problem is to solve two
penalized regression problems iteratively. Let's call them problem u and problem v, which give
improving estimates of the right singular vector, \emph{u}, and the left singular vector, \emph{v}, respectively.
For each regression problem, we can select the optimal parameters
based on BIC.

The nested BIC search is essentially two 2-D searches. We start from SVD solutions, and then find the optimal
parameters for problem u, given current estimate of \emph{v}. Using the result from previous step, update
current estimate of \emph{u}, and then do the same thing for problem v,
that is, to find the optimal parameters for problem v given current estimate of \emph{u}. Repeat
the above until convergence or the maximal number of iterations has been reached.

Users are welcome to refer to section 3.1: Selection of Regularization Parameters
in the paper cited below.}
}
\value{
A \code{moma_sparsity_type} object, which is an empty list.
}
\description{
Use this function to set the penalty function to fused lasso
\deqn{\lambda \sum | x_{i} - x_{i-1} |,}
where \eqn{\lambda} is set by the \code{lambda} argument below.
}
\references{
Tibshirani, Robert, et al. "Sparsity and Smoothness via the Fused Lasso."
Journal of the Royal Statistical Society: Series B (Statistical Methodology) 67.1 (2005): 91-108.
\doi{10.1111/j.1467-9868.2005.00490.x}.

Hoefling, H. (2010). A path algorithm
for the fused lasso signal approximator. Journal of Computational and Graphical
 Statistics, 19(4), 984-1006. \doi{10.1198/jcgs.2010.09208}.

Johnson, N. A. (2013). A dynamic programming algorithm for the
fused lasso and l 0-segmentation. Journal of Computational and Graphical
Statistics, 22(2), 246-260. \doi{10.1080/10618600.2012.681238}.
}
